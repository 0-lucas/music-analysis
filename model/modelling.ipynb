{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:16.745550Z",
     "start_time": "2024-07-28T18:33:16.736969Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as sm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "from utils import get_music_time_series, read_csv_properly, test_augmented_df, get_optimized_arima\n",
    "\n",
    "sns.set_palette(\"viridis\")\n",
    "np.random.seed(42)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was already explored in the [previous notebook](eda.ipynb). Now let's try to create a statistical/ Machine Learning model that can capture and inference from the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:17.993509Z",
     "start_time": "2024-07-28T18:33:16.843136Z"
    }
   },
   "source": [
    "music = get_music_time_series(read_csv_properly(\"scrobbles.csv\"), \"W\")[\"track\"]\n",
    "sns.lineplot(music);"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go basically two routes: the statistical autoregressive / ARIMA approach, or the ML \"linear regression is all you need\" mentality. Let's test both because it's fun!\n",
    "\n",
    "We saw in the EDA notebook that our series is statistically not stationary. To make it stationary, we can differentiate it.\n",
    "\n",
    "A simple way to differentiate a time series, is subtracting the previous value for every value in the series. This is the First Order Differentiation\n",
    "\n",
    "- ##### $x_t' = x_t - x_{t-1}$\n",
    "\n",
    "But why do this in the first place? Most statistical and Machine Learning models assumes that the data is stationary. This works because we are trying to model the *rate of change* instead of the actual absolute value. If we have a good model of the rate of change, we can predict from the last known absolute value.\n",
    "\n",
    "This is different from most i.i.d. data, where we try to predict the actual absolute values. e.g. house marret values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:18.457366Z",
     "start_time": "2024-07-28T18:33:17.994949Z"
    }
   },
   "source": [
    "# Differentiation loses a data instance. That's why we are slicing music.index\n",
    "music_diff = pd.Series(data=np.diff(music, n=1), index=music.index[1:])\n",
    "\n",
    "test_augmented_df(music_diff)\n",
    "\n",
    "sns.lineplot(music_diff) \n",
    "plt.title(\"Differenced series\");"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the p-value is low (because of the lags used for testing the original series).\n",
    "\n",
    "We can reject the null hypothesis and conclude that now the series is stationary. We can now start modelling!\n",
    "\n",
    "But before that, let's split the data, so that we avoid biased estimators and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:18.464109Z",
     "start_time": "2024-07-28T18:33:18.458809Z"
    }
   },
   "source": [
    "train, test = train_test_split(music_diff, random_state=42, shuffle=False, test_size=4)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a ARIMA model we check how it performs.\n",
    "\n",
    "ARIMA stands for AutoRegressive Integrated Moving Average, combining both AR and MA models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:19.014481Z",
     "start_time": "2024-07-28T18:33:18.473684Z"
    }
   },
   "source": [
    "# Setting an ARIMA model\n",
    "model = ARIMA(train, order=(1,1,1))\n",
    "model = model.fit()\n",
    "\n",
    "plot = sns.lineplot(train[120:], label=\"Train data\")\n",
    "sns.lineplot(model.forecast(len(test)), label=\"Forecast\", ax=plot, color=\"blue\")\n",
    "sns.lineplot(test, label=\"Test data\", ax=plot, color=\"tomato\")\n",
    "plt.title(\"Applying ARIMA\")\n",
    "\n",
    "# Getting ARIMA model metrics\n",
    "print(model.summary())"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of ARIMA makes it weak for predicting seasonality and cyclic components. AIC is quite high and the prediction doesn't capture the seasonality.\n",
    "\n",
    "Either way ARIMA is really cool and was the basis of time series forecast for a long time! If you really like the stats route, you can check out SARIMAX models.\n",
    "\n",
    "Autoregression really isn't the way to go, and we saw it in the EDA.\n",
    "\n",
    "Let's go the ML route then!\n",
    "\n",
    "\n",
    "Let's get the original data, to try to model each component of a time series: trend, season and cyclic / residuals.\n",
    "\n",
    "We can define a time series as a function of these components, in a additive or multiplicative way.\n",
    "$$\n",
    "\\text{TS} = \\text{trend} + \\text{seasonality} + \\text{cycles} \\pm \\text{random error}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:19.485780Z",
     "start_time": "2024-07-28T18:33:19.017461Z"
    }
   },
   "source": [
    "# Redefining train-test split for not differenciated data.\n",
    "train, test = train_test_split(music, random_state=42, shuffle=False, test_size=4)\n",
    "\n",
    "# Setting a trend line for the date index, with a constant feature.\n",
    "dp = DeterministicProcess(\n",
    "\ttrain.index, constant=True, drop=True, order=2\n",
    ")\n",
    "\n",
    "# Using Linear Regression to model trend component\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(dp.in_sample(), train.values)\n",
    "\n",
    "trend_pred = pd.Series(model.predict(dp.in_sample()), index=train.index)\n",
    "trend_forecast = pd.Series(model.predict(dp.out_of_sample(4)), index=test.index)\n",
    "\n",
    "plot = sns.lineplot(train, label=\"Original data\", color=\"silver\")\n",
    "sns.lineplot(trend_pred, label=\"Modeled trend\")\n",
    "plt.title(\"Modelling trend with Linear Regression\");"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a reasonable trend to our data. We could make it more flexible by increase the order of the linear regression function.\n",
    "\n",
    "In fact, linear regression here is used solely for trend modelling. It doesn't comply to some assumptions made to time series linear regression, and it shouldn't be considered an unbiased estimator. Because we are using it just for forecasting, and not causality, we can discard these assumptions for now.\n",
    "\n",
    "Now, let's differentiate our data by subtracting the trend prediction from it. In this case, the regression line."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:33:19.868794Z",
     "start_time": "2024-07-28T18:33:19.486852Z"
    }
   },
   "source": [
    "seasonal_train =  train - trend_pred\n",
    "test_augmented_df(seasonal_train)\n",
    "\n",
    "\n",
    "sns.lineplot(seasonal_train)\n",
    "plt.title(\"Data without trend component\");"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a augmented Dickey-Fuller test, we can statistically determine our differentiated data is stationary. We can now model the next component: **seasonality**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:34:13.384773Z",
     "start_time": "2024-07-28T18:33:19.869801Z"
    }
   },
   "source": [
    "dp = DeterministicProcess(\n",
    "\tseasonal_train.index,\n",
    "\torder=0,\n",
    "\tseasonal=True,\n",
    "\tdrop=True,\n",
    "\tadditional_terms=\n",
    "\t\t[CalendarFourier(\"ME\", 4), CalendarFourier(\"YE\", 4)]\n",
    ")\n",
    "\n",
    "# Setting parameters for a hyperparameter optimization\n",
    "parameters = {\n",
    "\t\"max_depth\": [5, 10 ,15, 20],\n",
    "\t\"n_estimators\": [10, 25, 50, 100, 150, 200],\n",
    "\t\"warm_start\": [True, False]\n",
    "}\n",
    "\n",
    "# Using a specific time series oriented cross validation split\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "# Setting base model and retrieving the best estimator from grid search\n",
    "model = RandomForestRegressor(n_jobs=-1, criterion=\"absolute_error\")\n",
    "grid_search = GridSearchCV(model, parameters, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "model = grid_search.fit(dp.in_sample(), seasonal_train.values).best_estimator_\n",
    "\n",
    "# Getting predictions in sample\n",
    "seasonal_pred = pd.Series(model.predict(dp.in_sample()), index=seasonal_train.index)\n",
    "\n",
    "# Getting forecast out of sample\n",
    "seasonal_forecast = pd.Series(model.predict(dp.out_of_sample(4)), index=test.index)\n",
    "\n",
    "plot = sns.lineplot(seasonal_pred,label=\"Seasonality Prediction\")\n",
    "sns.lineplot(seasonal_train, label=\"Differenced data\", color=\"silver\", alpha=0.5, ax=plot)\n",
    "plt.title(\"Seasonality modelling\");"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the same differentiation logic to try and model the cyclic component before determining the residual as random errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:34:13.733311Z",
     "start_time": "2024-07-28T18:34:13.387461Z"
    }
   },
   "source": [
    "# Differentiating and lagging the series to analyze the residual\n",
    "cyclic_diff = seasonal_train - seasonal_pred\n",
    "cyclic_shifted = cyclic_diff.shift(1).dropna()\n",
    "\n",
    "corr, _ = pearsonr(cyclic_diff[1:], cyclic_shifted)\n",
    "\n",
    "plt.text(-50, -200, f\"Pearson correlation: {corr:.2f}\", fontsize=14)\n",
    "sns.regplot(y=cyclic_diff[1:], x=cyclic_shifted);"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation of cyclic events based on the lag method approach doesn't really work out. It can be interpreted as a moderate correlation at most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:34:54.495065Z",
     "start_time": "2024-07-28T18:34:52.384527Z"
    }
   },
   "source": [
    "residuals = music - (trend_pred + seasonal_pred)\n",
    "\n",
    "plt.subplots(figsize=(20,10), sharex=True, sharey=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(311)\n",
    "ax1 = sns.lineplot(music, label=\"Original data\", color=\"silver\", alpha=0.5)\n",
    "sns.lineplot(trend_pred, label=\"Modeled trend\", ax=ax1)\n",
    "\n",
    "plt.subplot(312)\n",
    "ax2 = sns.lineplot(seasonal_pred,label=\"Seasonality Prediction\")\n",
    "sns.lineplot(seasonal_train, label=\"Differenced data\", color=\"silver\", alpha=0.5, ax=ax2)\n",
    "\n",
    "plt.subplot(313)\n",
    "sns.lineplot(residuals, label=\"Residuals\", color=\"silver\");\n"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:34:59.697149Z",
     "start_time": "2024-07-28T18:34:59.124826Z"
    }
   },
   "source": [
    "prediction_in_sample = trend_pred + seasonal_pred\n",
    "prediction_forecast = trend_forecast + seasonal_forecast\n",
    "\n",
    "sns.lineplot(music, color=\"silver\", label=\"Original data\")\n",
    "sns.lineplot(prediction_in_sample, color=\"blue\", label=\"In sample prediction\")\n",
    "sns.lineplot(prediction_forecast, color=\"red\", label=\"Forecast\");"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we saw it visually, let's take a look at some metrics. This is where the actual comparison between estimation methods and validation happens."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:35:02.628420Z",
     "start_time": "2024-07-28T18:35:02.617608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"In sample Mean Absolute Error: {mean_absolute_error(train, prediction_in_sample):.2f} \\n\"\n",
    "\t  f\"In sample Median Absolute Error: {median_absolute_error(train, prediction_in_sample):.2f}\")\n",
    "\n",
    "print(f\"Forecast Mean Absolute Error: {mean_absolute_error(test, prediction_forecast):.2f} \\n\"\n",
    "\t  f\"Forecast Median Absolute Error: {median_absolute_error(test, prediction_forecast):.2f}\")"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In defense of myself, I had some terrible days in the past years when I'd cope listening to music. This may explain the high variance and outliers in training data that was not captured by the model. Surprisingly, the forecast was more precise, but that is probably due to smaller variance. In fact, the distribution changes A LOT depending on each time slice you analyze. Some better preprocessing or more outlier-resistance estimators would probably perform better...\n",
    "\n",
    "The model is not perfect, but it's honest predictions...Let's roll with it!\n",
    "\n",
    "#### I've consolidated the entire model in [model.py](model.py)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T18:35:08.056280Z",
     "start_time": "2024-07-28T18:35:07.552018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Little showcase of consolidated model\n",
    "from model import MusicPredictor\n",
    "\n",
    "x = MusicPredictor()\n",
    "x.fit(train)\n",
    "x.predict(4)"
   ],
   "execution_count": 45,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
